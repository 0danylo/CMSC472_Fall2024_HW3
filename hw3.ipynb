{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Instructions\n",
    "\n",
    "1. Enter your Name and UID in the provided space.\n",
    "2. Do the assignment in the notebook itself.\n",
    "3. You are free to use Google Colab.\n",
    "4. Upload to Google Drive.\n",
    "5. Now, enter the Google Drive link in the provided space. (you can do this by opening the iPython notebook uploaded using Google Collab).\n",
    "6. Submit the assignment to Gradescope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name:  **Name Here**  \n",
    "UID:  **UID Here**\n",
    "\n",
    "Link to Google Drive : **Link Here (make sure it works)**\n",
    "\n",
    "Provide your code at the appropriate placeholders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define your hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "learning_rate = 0.001\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset Description**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing instructions:** As usual, you reshape and standardize the images before feeding them to the network.\n",
    "\n",
    "<img src=\"imvectorkiank.png\" style=\"width:450px;height:300px;\">\n",
    "\n",
    "<caption><center> <u>Figure 1</u>: Image to vector conversion. <br> </center></caption><br>\n",
    "\n",
    "We are going to get help from  *torch.utils.data.Dataset*, which is an abstract class representing a dataset. Our *birdvnonbird* class should inherit Dataset and override the following methods:\n",
    "\n",
    "* __\\_\\_len\\_\\___ : returns the size of the dataset\n",
    "* __\\_\\_getitem\\_\\___ : helps indexing and creating the batches.\n",
    "\n",
    "**Exercise:** Implement the *birdvnonbird* class that will help to load your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "class birdvnonbird(Dataset):\n",
    "    \"\"\"\n",
    "    Helps to load the dataset\n",
    "    \n",
    "    Arguments:\n",
    "    file_name -- Data file name\n",
    "    mode -- There are three modes possible: train, test, evaluate\n",
    "    \n",
    "    Returns:\n",
    "    train mode -- x_train, y_train\n",
    "    test mode -- x_test, y_test\n",
    "    evaluate mode -- x_eval\n",
    "    \n",
    "    x has the shape (batch_size, input_layer_size)\n",
    "    y has the shape (batch_size, output_layer_size)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, file_name, mode = 'train'):\n",
    "        self.mode = mode\n",
    "        self.file_name = file_name\n",
    "        \n",
    "        #Load the data file\n",
    "        ### START CODE HERE ###\n",
    "        data = h5py.File(file_name, \"r\")\n",
    "        data = torch.load(file_name, \"r\")\n",
    "        print(data)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        #Extract x,y for training/testing/evaluation.\n",
    "        if self.mode == 'train':\n",
    "            self.set_x = np.array(data[\"train_set_x\"])\n",
    "            self.set_y = np.array(data[\"train_set_y\"])\n",
    "            self.classes = np.array(data[\"list_classes\"][:])\n",
    "            self.set_y = self.set_y.reshape((self.set_y.shape[0],1))\n",
    "            \n",
    "        if self.mode == 'test':\n",
    "            self.set_x = np.array(data[\"test_set_x\"])\n",
    "            self.set_y = np.array(data[\"test_set_y\"])\n",
    "            self.classes = np.array(data[\"list_classes\"][:])\n",
    "            self.set_y = self.set_y.reshape((self.set_y.shape[0],1))\n",
    "            \n",
    "        if self.mode == 'evaluate':\n",
    "            self.set_x = np.array(data[\"evaluate_set_x\"])\n",
    "            self.classes = np.array(data[\"list_classes\"][:])\n",
    "            \n",
    "        self.set_x = (self.set_x.reshape(self.set_x.shape[0], -1))/255. \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.set_x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == 'train':\n",
    "            ## Return x_train, y_train\n",
    "            return torch.Tensor(self.set_x[idx]),torch.Tensor(self.set_y[idx])\n",
    "        if self.mode == 'test':\n",
    "            ## Return x_test, y_test\n",
    "            return torch.Tensor(self.set_x[idx]),torch.Tensor(self.set_y[idx])\n",
    "        if self.mode == 'evaluate':\n",
    "            ## Return only x_eval\n",
    "            return torch.Tensor(self.set_x[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (file signature not found)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-313-d074d55aa573>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"toy_dataset.pth\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbirdvnonbird\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbirdvnonbird\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-311-01817b0df109>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file_name, mode)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m#Load the data file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m### START CODE HERE ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    404\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrdcc_nslots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrdcc_nbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrdcc_w0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m                 fid = make_fid(name, mode, userblock_size,\n\u001b[0m\u001b[1;32m    407\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                                swmr=swmr)\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (file signature not found)"
     ]
    }
   ],
   "source": [
    "train_file=\"data/train_catvnoncat.h5\"\n",
    "test_file=\"data/test_catvnoncat.h5\"\n",
    "\n",
    "data_file = \"toy_dataset.pth\"\n",
    "train_set = birdvnonbird(data_file,\"train\")\n",
    "test_set = birdvnonbird(test_file,\"test\")\n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "inputs,labels = next(iter(train_dataloader))\n",
    "print(inputs.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Architecture of your model\n",
    "\n",
    "Now that you are familiar with the dataset, it is time to build a deep neural network to distinguish bird images from non-bird images.\n",
    "\n",
    "###  2-layer neural network\n",
    "\n",
    "<!-- <img src=\"2layerNN_kiank.png\" style=\"width:650px;height:400px;\">\n",
    "<caption><center> <u>Figure 2</u>: 2-layer neural network. <br> The model can be summarized as: ***INPUT -> LINEAR -> RELU -> LINEAR -> SIGMOID -> OUTPUT***. </center></caption> -->\n",
    "\n",
    "<!-- <u>Detailed Architecture of figure 2</u>: -->\n",
    "\n",
    "- The input is a (64,64,3) image which is flattened to a vector of size $(12288,1)$. \n",
    "- The corresponding vector: $[x_0,x_1,...,x_{12287}]^T$ is then multiplied by the weight matrix $W^{[1]}$ of size $(n^{[1]}, 12288)$.\n",
    "- You then add a bias term and take its relu to get the following vector: $[a_0^{[1]}, a_1^{[1]},..., a_{n^{[1]}-1}^{[1]}]^T$.\n",
    "- You multiply the resulting vector by $W^{[2]}$ and add your intercept (bias). \n",
    "- Finally, you take the sigmoid of the result. If it is greater than 0.5, you classify it to be a cat.\n",
    "\n",
    "\n",
    "###  General methodology\n",
    "\n",
    "As usual you will follow the Deep Learning methodology to build the model:\n",
    "    1. Initialize parameters / Define hyperparameters\n",
    "    2. Loop for num_iterations:\n",
    "        a. Forward propagation\n",
    "        b. Compute loss function\n",
    "        c. Backward propagation\n",
    "        d. Update parameters (using parameters, and grads from backprop) \n",
    "    4. Use trained parameters to predict labels\n",
    "\n",
    "Let's now implement those the model! \n",
    "\n",
    "$$\\color{red}{text here}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input a feature of dimenstion 1024.\n",
    "        self.fc1 = nn.Linear(64*64*3, 7)\n",
    "        self.fc2 = nn.Linear(7, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):     #This is the forward propagation function which will be called everytime during forward pass\n",
    "        #x is the input that we will give in the network.\n",
    "        x = self.fc1(x) #Passsing the function through the first Fully connected layer\n",
    "        x = torch.relu(x) #Applying the sigmoid activation to the outputof the first fc layer\n",
    "        x = self.fc2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define your hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "learning_rate = 0.001\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize the Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# for idx, batch in enumerate(test_dataloader):\n",
    "#     inputs,labels = batch\n",
    "#     print(inputs.shape,labels.shape,labels)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Define the optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()  #making an instance of the network\n",
    "optim = torch.optim.SGD(model.parameters(), lr = learning_rate)  #model.paramters() gives all the trainable paramters. \n",
    "loss_function = nn.BCELoss() #many people call this the criterion also."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Defining the for loop for train and validation phase\n",
    "\n",
    "### In each the phases certain things one has to be careful of:\n",
    "\n",
    "- Training Phase:\n",
    "  - Make sure the model is in train mode. That is ensured by `model.train()`\n",
    "\n",
    "  - While looping over instances of a batch, make sure the graidents are always set to zero before calling the backward function. That's done by `optim.zero_grad()`. If this is not done, the gradients get accumulated.\n",
    "\n",
    "  - Call the backward function on the loss by `loss.backward()` so that the loss get back propagated.\n",
    "\n",
    "  - Call the step function of the optimiser to update the weights of the network. This is done by `optim.step()`\n",
    "\n",
    "- Validation/Testing Phase\n",
    "  - Make sure your model is in eval mode. This makes the model deterministic rather than probabilistic. This is ensured by `model.eval()`\n",
    "  - As we don't need any gradients doing our validation/ testing phase, we can esnure that they are not calculated by defining a block with `torch.no_grad()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0, Train loss: 0.07297294453850814 , Train Acc: 65.55023923444976, Val loss: 0.6431166678667068\n",
      "Epoch : 100, Train loss: 0.07406534467424665 , Train Acc: 65.55023923444976, Val loss: 0.6512221843004227\n",
      "Epoch : 200, Train loss: 0.06880053452083043 , Train Acc: 65.55023923444976, Val loss: 0.6609604209661484\n",
      "Epoch : 300, Train loss: 0.0662411672196218 , Train Acc: 65.55023923444976, Val loss: 0.6675984412431717\n",
      "Epoch : 400, Train loss: 0.0600972996492471 , Train Acc: 65.55023923444976, Val loss: 0.6766654998064041\n",
      "Epoch : 500, Train loss: 0.061264866430844576 , Train Acc: 65.55023923444976, Val loss: 0.6835901960730553\n",
      "Epoch : 600, Train loss: 0.059312045840280395 , Train Acc: 65.55023923444976, Val loss: 0.6919058561325073\n",
      "Epoch : 700, Train loss: 0.05377317241592599 , Train Acc: 65.55023923444976, Val loss: 0.6964394822716713\n",
      "Epoch : 800, Train loss: 0.05292575926120792 , Train Acc: 65.55023923444976, Val loss: 0.7021104469895363\n",
      "Epoch : 900, Train loss: 0.05079331293901695 , Train Acc: 65.55023923444976, Val loss: 0.7065596133470535\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "\n",
    "    #Training phase\n",
    "    model.train()  #Setting the model to train phase\n",
    "    train_loss = []\n",
    "    train_correct = 0.\n",
    "    train_count = 0.\n",
    "    train_acc = 0.\n",
    "    test_acc = 0.\n",
    "    \n",
    "    for idx, batch in enumerate(train_dataloader):\n",
    "        inputs,labels = batch\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            prd = torch.argmax(outputs,1)\n",
    "            train_correct = train_correct + torch.sum(prd==labels.squeeze(1)).item()  \n",
    "            train_count = train_count + prd.shape[0]\n",
    "        loss = loss_function(outputs,labels)\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "        optim.zero_grad()  #Setting the gradients to be zero\n",
    "        loss.backward()  #This propagates the loss backward to the entire network, hence calculating the gradients of each weight\n",
    "        optim.step()  #Updates the paramters with the graidents calculated in the previous step\n",
    "    train_acc = train_correct/train_count*100\n",
    "    #Validation phase\n",
    "    model.eval()  #Setting the model to eval mode, hence making it deterministic.\n",
    "    val_loss = []\n",
    "\n",
    "    for idx, batch in enumerate(test_dataloader):\n",
    "        with torch.no_grad():   #Does not calulate the graidents, as in val phase its not needed. Saves on memory.\n",
    "            inputs,labels = batch\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs,labels)\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "    if epoch%100==0:\n",
    "        print(\"Epoch : {}, Train loss: {} , Train Acc: {}, Val loss: {}\".format(epoch, np.mean(train_loss), train_acc, np.mean(val_loss)))\n",
    "\n",
    "  #You can caluculate the accuracy too, I got too lazy to write the code for it.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 209) (1, 209)\n",
      "Accuracy: 0.9999999999999998\n"
     ]
    }
   ],
   "source": [
    "predictions_train = predict(train_x, train_y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50) (1, 50)\n",
      "Accuracy: 0.72\n"
     ]
    }
   ],
   "source": [
    "predictions_test = predict(test_x, test_y, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise:***\n",
    "Identify the hyperparameters in the model and For each hyperparameter\n",
    "- Briefly explain its role\n",
    "- Explore a range of values and describe their impact on (a) training loss and (b) test accuracy\n",
    "- Report the best hyperparameter value found.\n",
    "\n",
    "Note: Provide your results and explanations in the answer for this question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Type your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  8. Results Analysis\n",
    "\n",
    "First, let's take a look at some images the 2-layer model labeled incorrectly. This will show a few mislabeled images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_result(image_ids, probabilities):\n",
    "    if image_ids.shape != (50, ) or probabilities.shape != (50, ):\n",
    "        'make sure your arrays are correct shape'\n",
    "    else:\n",
    "        results_obj = {}\n",
    "        results_obj['images'] = image_ids\n",
    "        results_obj['probabilities'] = probabilities\n",
    "        torch.save(results_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_result(image_ids, probabilities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
