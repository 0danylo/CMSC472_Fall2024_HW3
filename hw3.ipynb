{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Instructions\n",
    "\n",
    "1. Enter your Name and UID in the provided space.\n",
    "2. **Download the data file [here](https://umd.instructure.com/files/80453154/download?download_frd=1). Make sure that you are logged in to ELMS before downloading.**\n",
    "3. Do the assignment in the notebook itself.\n",
    "4. You are free to use Google Colab.\n",
    "5. Upload to Google Drive.\n",
    "6. Now, enter the Google Drive link in the provided space. (you can do this by opening the iPython notebook uploaded using Google Collab).\n",
    "7. Feel free to refer to the Neural Network tutorial and the PyTorch tutorial included with the TA lecture material. \n",
    "8. Submit the assignment to Gradescope, and a results file to ELMS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Overview\n",
    "\n",
    "In this assignment, you will learn PyTorch while trying to create a bird vs. non-bird classifier. More specifically, you will\n",
    "1. Write a Dataset class in PyTorch.\n",
    "2. Define a neural network in PyTorch.\n",
    "3. Implement training and testing loops in PyTorch.\n",
    "4. Find a way to improve the performance of your network above our baseline.\n",
    "5. Compete for the best performance on a held-out testset.\n",
    "6. Provide both a written description and a graphical justification of your approach.\n",
    "\n",
    "# Challenge Description\n",
    "\n",
    "This assignment is a challenge. How exciting! As in Homework 2, this is a binary classification problem, where your network will learn to discriminate birds from other things. We have curated a special dataset, new to this assignment, consisting of birds and non-birds. As before, we provide you with train and validation images and labels. However, for the test set, we only give you test images! (No labels!) Your goal is to try to get the best classification performance in the class on these test images.\n",
    "\n",
    "# Leaderboard \n",
    "\n",
    "Things just got more interesting! The top-10 testset accuracies in class will be displayed in a leaderboard which will be updated twice:\n",
    "1. Intermediate round: Submit your `challenge_3_results_nickname.pth` from exercise 8 to ELMS by **TBD 5.00pm**. Find the nickname rules under section 8.\n",
    "2. Final round: With your final submission.\n",
    "\n",
    "**Submissions with the top-3 accuracies in class on the *testset* during the *final round* will win lucrative ~~cash prizes~~ bonus points!**\n",
    "\n",
    "* **Winner:** 20 bonus points \n",
    "* **Runners-up:** 10 bonus points \n",
    "* **Second Runners-up:** 5 bonus points \n",
    "\n",
    "**Submissions with the top-3 accuracies in class on the *testset* during the *intermediate round* will win 5 bonus points each!**\n",
    "\n",
    "\n",
    "# Challenge Rules\n",
    "\n",
    "1. You **MUST** submit a results file on ELMS. Details are provided in Section 8 of this notebook.\n",
    "2. You **MAY NOT** deviate from neither of the **number of layers (2)** nor the **number of parameters (hidden layer size = 7)** in each layer from Assignment 2. Feel free to ask questions by making a *Public* post in either Piazza or Slack if you are unsure if a given change is allowed or not.\n",
    "3. You **MAY NOT** attempt to manually label the test images. We want to evaluate the performance of your neural classification network, not your classification ability.\n",
    "4. You should try to achieve at least **75%** validation accuracy (Baseline)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name:  **Name Here**  \n",
    "UID:  **UID Here**\n",
    "\n",
    "Link to Google Drive : **Link Here (make sure it works)**\n",
    "\n",
    "Provide your code at the appropriate placeholders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset Description**: \n",
    "\n",
    "In the provided torch file, you will find an object with the following keys, with corresponding shapes:\n",
    "\n",
    "`train_images`: (400,64,64,3)\n",
    "\n",
    "`train_labels`: (400)\n",
    "\n",
    "`val_images`: (400,64,64,3)\n",
    "\n",
    "`val_labels`: (400)\n",
    "\n",
    "`test_images`: (400,64,64,3)\n",
    "\n",
    "The values corresponding to `train_images`, `val_images`, and `test_images` are separate tensors, each containing 400 64 x 64 images. Each set of 400 images is comprised of 200 birds and 200 non-birds.\n",
    "\n",
    "The values corresponding to `train_labels` and `val_labels` are tensors containing a list of integers, 1 or 0, to indicate whether an image is a bird (1) or non-bird (0). These are parallel to their corresponding image tensor. That is, the first label in `train_labels` corresponds to the first entry of `train_images`. \n",
    "\n",
    "Below, we give a quick peak into what the training set looks like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.load('toy_dataset.pth')\n",
    "\n",
    "print('train image shape:', dataset['train_images'].shape)\n",
    "print('train labels shape:', dataset['train_labels'].shape)\n",
    "\n",
    "\n",
    "def plotToy(figName, images):\n",
    "    fig = plt.figure(figsize=(20.5, 12)) \n",
    "    nrow = 5\n",
    "    ncol = 5\n",
    "    gs = gridspec.GridSpec(nrow, ncol,\n",
    "         wspace=0.0, hspace=0.05, \n",
    "         top=1.-0.5/(nrow+1), bottom=0.5/(nrow+1), \n",
    "         left=0.5/(ncol+1), right=1-0.5/(ncol+1))\n",
    "\n",
    "    i = 0\n",
    "    for j in range(images.shape[0]):\n",
    "        rgb_im = images[j].numpy().astype(np.uint8)\n",
    "\n",
    "        ax = plt.subplot(gs[int(i / ncol), i % ncol])\n",
    "        ax.imshow(rgb_im)\n",
    "        ax.axis('off')\n",
    "        i += 1\n",
    "\n",
    "    plt.savefig(figName + '.png', bbox_inches='tight', pad_inches=0)\n",
    "    \n",
    "plotToy('toy_plot', dataset['train_images'][:25])\n",
    "print(\"labels:\", dataset['train_labels'][:25].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PyTorch Dataset**: \n",
    "\n",
    "You will now implement a torch dataset that will be compatible with the dataloader code in Section 5. We give you part of the outline below, feel free to use the documentation (https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#Dataset) or a tutorial for more details.\n",
    "\n",
    "<img src=\"imvectorkiank.png\" style=\"width:450px;height:300px;\">\n",
    "\n",
    "<caption><center> <u>Figure 1</u>: Image to vector conversion. <br> </center></caption> <br>\n",
    "\n",
    "**Exercise 1:** Complete the *BirdVNonBirdDataset* class which inherits the *torch.utils.data.Dataset* class and override the __\\_\\_len\\_\\___ and __\\_\\_getitem\\_\\___  methods. \n",
    "\n",
    "**Important:** Don't forget to reshape and standardize images as necessary before feeding them to the network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdVNonBirdDataset(Dataset):\n",
    "    def __init__(self, file_name, mode = 'train'):\n",
    "        self.file_name = file_name\n",
    "        self.mode = mode\n",
    "        \n",
    "        ## TODO: read data from toy_dataset.pth and set up the code such that: \n",
    "        ##      - len will return the length\n",
    "        ##      - __getitem__ will return the images, with labels depending on the mode\n",
    "        ##      - don't forget the mode- load train images if train, test if test, etc.\n",
    "\n",
    "    def __len__(self):\n",
    "        ## TODO: Return number of images in the dataset\n",
    "        return ##\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ## return image, and, if not test mode, the label at the given index\n",
    "        ## valid formats include:\n",
    "        ## {'image': image, 'label': label}\n",
    "        ## image, label\n",
    "        ## where image and label have both been converted to torch.Tensors\n",
    "        return ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Architecture of your model\n",
    "\n",
    "Now that you are familiar with the dataset, it is time to build a deep neural network to distinguish bird images from non-bird images.\n",
    "\n",
    "###  2-layer neural network\n",
    "\n",
    "\n",
    "We are going to use the same architecture as we used in HW02. The model can be summarized as: \n",
    "\n",
    "<center> <b> INPUT -> LINEAR -> RELU -> LINEAR -> SIGMOID -> OUTPUT </b> </center>\n",
    "\n",
    "\n",
    "###  General methodology\n",
    "\n",
    "As usual you will follow the Deep Learning methodology to build the model:\n",
    "\n",
    "    1. Initialize parameters / Define hyperparameters\n",
    "    2. Loop for num_iterations:\n",
    "        a. Forward propagation\n",
    "        b. Compute loss function\n",
    "        c. Backward propagation an update parameters (using parameters, and grads from backprop) \n",
    "    3. Use trained parameters to predict labels\n",
    "\n",
    "Let's now implement the model!\n",
    "\n",
    "**Exercise 2:** Implement the neural network model.\n",
    "\n",
    "**Important:** You MAY NOT deviate from neither of the **number of layers (2)** nor the **number of parameters (hidden layer size = 7)** in each layer from Assignment 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        ## TODO: define the model\n",
    "        ## REMEMBER not to change the number of layers nor the number of parameters per layer\n",
    "        \n",
    "\n",
    "    def forward(self, x):     \n",
    "        #x is the input that we will give in the network.\n",
    "        ## TODO: Implement the forward propagation function which will be called everytime during forward pass\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define your hyperparamters\n",
    "\n",
    "**Exercise 3:** Please make sure your final hyperparameters are in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "learning_rate = 0.001\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize the Dataloaders\n",
    "\n",
    "**Exercise 4:** Initialize you dataloaders with instances of the dataset you defined in Section 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = BirdVNonBirdDataset(...) ## TODO\n",
    "val_set = BirdVNonBirdDataset(...) ## TODO\n",
    "test_set = BirdVNonBirdDataset(...) ## TODO\n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(test_set, batch_size=batch_size, shuffle=False) #DO NOT change the shuffle=False here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Define the optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()  #making an instance of the network\n",
    "optim = torch.optim.SGD(model.parameters(), lr = learning_rate)  #model.paramters() gives all the trainable paramters. \n",
    "loss_function = nn.BCELoss() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Defining the for loop for train and validation phase\n",
    "\n",
    "### In each the phases certain things one has to be careful of:\n",
    "\n",
    "- Training Phase:\n",
    "  - Make sure the model is in train mode. That is ensured by `model.train()`\n",
    "\n",
    "  - While looping over instances of a batch, make sure the graidents are always set to zero before calling the backward function. That's done by `optim.zero_grad()`. If this is not done, the gradients get accumulated.\n",
    "\n",
    "  - Call the backward function on the loss by `loss.backward()` so that the loss get back propagated.\n",
    "\n",
    "  - Call the step function of the optimiser to update the weights of the network. This is done by `optim.step()`\n",
    "\n",
    "- Validation/Testing Phase\n",
    "  - Make sure your model is in eval mode. This makes the model deterministic rather than probabilistic. This is ensured by `model.eval()`\n",
    "  - As we don't need any gradients doing our validation/ testing phase, we can esnure that they are not calculated by defining a block with `torch.no_grad()`\n",
    "  \n",
    "**Exercise 5:** Implement the training and validation phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "\n",
    "    #Training phase\n",
    "    model.train()  #Setting the model to train phase\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    train_acc = 0.\n",
    "    val_acc = 0.\n",
    "    \n",
    "    for idx, batch in enumerate(train_dataloader):\n",
    "        ## TODO: fetch inputs and labels from batch\n",
    "        ## TODO: pass inputs to model\n",
    "\n",
    "        ## TODO: call loss function\n",
    "        ## TODO: call backward\n",
    "        ## TODO: step optimizer\n",
    "\n",
    "        ## TODO: compute accuracy and track accuracy and loss\n",
    "\n",
    "        ## IMPORTANT: don't forget to call zero_grad on the optimizer between batches!!!\n",
    "\n",
    "    #Validation phase\n",
    "    model.eval()  #Setting the model to eval mode, hence making it deterministic.\n",
    "    for idx, batch in enumerate(val_dataloader):\n",
    "        with torch.no_grad():   #Does not calulate the graidents, as in val phase its not needed. Saves on memory.\n",
    "            ## TODO: similar to training, but no need to call backward, step, or zero_grad\n",
    "\n",
    "    ## DO NOT change the next two lines\n",
    "    if epoch%100==0:\n",
    "        print(\"Epoch : {}, Train loss: {} , Train Acc: {}, Val loss: {}, Val acc: {}\".format(epoch, np.mean(train_loss), train_acc, np.mean(val_loss), val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6:** Plot the training and validation loss curves here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### YOUR CODE FOR PLOTTING THE LOSS CURVES HERE ####\n",
    "#### FEEL FREE TO REUSE CODE FROM HW2 ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 7:** Report your best validation accuracy and the corresponding validation loss.\n",
    "\n",
    "**Important:** You should try to achieve at least **75%** validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_acc = 0.0\n",
    "corr_val_loss = 0.0\n",
    "\n",
    "## TODO: Report your best performance\n",
    "\n",
    "#DO NOT change the next line\n",
    "print(\"Best Val acc: {}, Corresponding Val loss: {}\".format(best_val_acc,  corr_val_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  8. Create challenge output file\n",
    "\n",
    "Here, we provide code for you to generate a submission file containing your test probabilities tensor for consideration in the challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_results_file(probabilities,nick_name):\n",
    "    if probabilities.shape[0] != 400:\n",
    "        'make sure your array is the correct shape'\n",
    "    else:\n",
    "        save_path = 'challenge_3_results_{}.pth'.format(nick_name)\n",
    "        results_obj = {}\n",
    "        results_obj['probabilities'] = probabilities\n",
    "        torch.save(results_obj, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 8:** Evaluate your trained model on the test set, and submit the results.\n",
    "\n",
    "**Important:** **DO NOT** shuffle the test set for any reason! Please remember to submit this file, `challenge_3_results_nickname.pth`, in ELMS.\n",
    "\n",
    "**Nick name rules:** Your nickname should be a single string consisting of alphanumeric characters, and your nickname should not reveal who you are! Try to be creative and unique as much as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nick_name = 'default' ## Change this to a unique nick name!\n",
    "\n",
    "model.eval()\n",
    "for idx, batch in enumerate(test_dataloader):\n",
    "    with torch.no_grad(): \n",
    "        ## TODO: Evaluate your trained model on the test set and generate `probabilities`.\n",
    "\n",
    "generate_results_file(probabilities,nick_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 9:*** Describe the changes you made from our default settings to improve your score by editing the \"answer\" text block below. Be sure to identify not only which changes you made, but also why you made them. (3-5 sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Answer: </span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 10:*** Provide at least 1 plot generated via matplotlib or a similar library proving that the changes you made improved performance on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### YOUR MATPLOTLIB CODE HERE ####\n",
    "#### FEEL FREE TO REUSE CODE FROM HW2 ####"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
